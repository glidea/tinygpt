# å¦‚ä½•è®¾è®¡ Tokenizer

## 1. Background

### 1.1 è®¡ç®—æœºå¦‚ä½•"ç†è§£"æ–‡æœ¬ï¼Ÿ

**æ ¸å¿ƒæŒ‘æˆ˜**ï¼šè®¡ç®—æœºåªè®¤è¯†æ•°å­—ï¼Œä½†äººç±»äº¤æµç”¨çš„æ˜¯æ–‡å­—ã€‚æˆ‘ä»¬éœ€è¦ä¸€åº§æ¡¥æ¢ï¼Œå°†æ–‡å­—æ˜ å°„æˆæ•°å­—ã€‚

#### é—®é¢˜åœºæ™¯
å‡è®¾æˆ‘ä»¬è¦è®­ç»ƒä¸€ä¸ª GPT æ¨¡å‹æ¥ç”Ÿæˆä¸­æ–‡æ–‡æœ¬ï¼Œæ¨¡å‹çš„è¾“å…¥/è¾“å‡ºéƒ½æ˜¯è¿™æ ·çš„ï¼š

```
è¾“å…¥ï¼ˆäººç±»ï¼‰: "ä½ å¥½ï¼Œä¸–ç•Œ"
       â†“ éœ€è¦è½¬æ¢
è¾“å…¥ï¼ˆæ¨¡å‹ï¼‰: [872, 1962, 234, 109, 675]  # ä¸€ä¸²æ•°å­—
       â†“ æ¨¡å‹è®¡ç®—
è¾“å‡ºï¼ˆæ¨¡å‹ï¼‰: [1123, 987, ...]
       â†“ éœ€è¦è½¬æ¢
è¾“å‡ºï¼ˆäººç±»ï¼‰: "å¾ˆé«˜å…´è®¤è¯†ä½ "
```

è¿™ä¸ª"æ–‡å­— â†” æ•°å­—"çš„æ˜ å°„è¿‡ç¨‹ï¼Œå°±æ˜¯ **åˆ†è¯å™¨ (Tokenizer)** è¦è§£å†³çš„é—®é¢˜ã€‚

#### ä¸ºä»€ä¹ˆä¸èƒ½ç®€å•åœ°"ä¸€ä¸ªæ±‰å­— = ä¸€ä¸ª ID"ï¼Ÿ

è¿™çœ‹èµ·æ¥å¾ˆç›´è§‚ï¼š`ä½ =1, å¥½=2, ä¸–=3, ç•Œ=4 ...`ï¼Œä½†ä¼šæœ‰ä¸¤ä¸ªå¤§é—®é¢˜ï¼š

1. **è¯è¡¨çˆ†ç‚¸**ï¼šä¸­æ–‡å¸¸ç”¨æ±‰å­— 3500 ä¸ªï¼ŒåŠ ä¸Šç”Ÿåƒ»å­—ã€æ ‡ç‚¹ã€è‹±æ–‡ã€æ•°å­—ã€emoji... è½»æ¾è¶…è¿‡ 10 ä¸‡ã€‚

2. **è¯­ä¹‰å‰²è£‚**ï¼šæ±‰å­—"ä¸–"å’Œ"ç•Œ"å•ç‹¬æ²¡ä»€ä¹ˆæ„ä¹‰ï¼Œä½†ç»„åˆæˆ"ä¸–ç•Œ"æ‰æ˜¯å®Œæ•´çš„è¯­ä¹‰å•å…ƒ
   - å¦‚æœæ‹†å¼€æˆä¸¤ä¸ª IDï¼Œæ¨¡å‹éœ€è¦é¢å¤–å­¦ä¹ å®ƒä»¬ä¹‹é—´çš„ç»„åˆå…³ç³»ï¼Œå¢åŠ å­¦ä¹ éš¾åº¦ã€‚

#### ç†æƒ³çš„åˆ†è¯æ–¹æ¡ˆæ˜¯ä»€ä¹ˆï¼Ÿ

ä¸€ä¸ªå¥½çš„åˆ†è¯å™¨éœ€è¦åœ¨ä¸¤ä¸ªæç«¯ä¸­æ‰¾åˆ°å¹³è¡¡ï¼š

| ç­–ç•¥                    | è¯è¡¨å¤§å°     | è¯­ä¹‰å®Œæ•´æ€§                       | çµæ´»æ€§                           |
| ----------------------- | ------------ | -------------------------------- | -------------------------------- |
| **æç«¯1ï¼šæŒ‰å­—æ‹†åˆ†**     | å°ï¼ˆå‡ åƒï¼‰   | âŒ å·®ï¼ˆ"ä¸–"+"ç•Œ" éœ€è¦é‡æ–°ç»„åˆï¼‰   | âœ… å¥½ï¼ˆä»»æ„æ–°å­—éƒ½èƒ½è¡¨ç¤ºï¼‰         |
| **æç«¯2ï¼šæŒ‰å®Œæ•´è¯æ‹†åˆ†** | å¤§ï¼ˆå‡ åä¸‡ï¼‰ | âœ… å¥½ï¼ˆ"ä¸–ç•Œ"æ˜¯å®Œæ•´è¯­ä¹‰ï¼‰         | âŒ å·®ï¼ˆæ–°è¯æ— æ³•è¡¨ç¤ºï¼Œå¦‚"å…ƒå®‡å®™"ï¼‰ |
| **âœ… BPE å­è¯åˆ†å‰²**      | ä¸­           | âœ… è¾ƒå¥½ï¼ˆå¸¸è§è¯å®Œæ•´ï¼Œç”Ÿåƒ»è¯æ‹†åˆ†ï¼‰ | âœ… å¥½ï¼ˆå­—èŠ‚çº§åˆ«å…œåº•ï¼‰             |

---

### 1.2 BPE çš„è§£å†³æ€è·¯ï¼šæ•°æ®é©±åŠ¨çš„"å‹ç¼©ç®—æ³•"

> **ç±»æ¯”**ï¼šBPE å°±åƒ ZIP å‹ç¼©ç®—æ³•ï¼Œæ‰¾åˆ°æ–‡æœ¬ä¸­é¢‘ç¹å‡ºç°çš„"æ¨¡å¼"å¹¶ç”¨æ›´çŸ­çš„ä»£ç æ›¿æ¢

#### æ ¸å¿ƒæ€æƒ³
ä¸æ–­æ‰¾å‡ºæ•°æ®ä¸­æœ€å¸¸å‡ºç°çš„"å­—èŠ‚å¯¹"ï¼Œç”¨ä¸€ä¸ªæ–°ç¬¦å·æ›¿æ¢å®ƒä»¬ï¼Œä»¥æ­¤å‹ç¼©æ•°æ®

#### è®­ç»ƒæµç¨‹

> âš ï¸ **æ³¨æ„**ï¼šä¸ºäº†ä¾¿äºç†è§£ï¼Œè¿™é‡Œä»"å­—"ï¼ˆæ±‰å­—ï¼‰çº§åˆ«æ¼”ç¤ºã€‚å®é™…çš„ Byte-Level BPE æ˜¯ä»**å­—èŠ‚**ï¼ˆByteï¼‰å¼€å§‹çš„

**æ­¥éª¤ 1ï¼šåˆå§‹åŒ–åŸºç¡€ Token**
```
æ–‡æœ¬: "ä½ å¥½ä½ å¥½ä¸–ç•Œ"
åˆå§‹æ‹†åˆ†: ["ä½ ", "å¥½", "ä½ ", "å¥½", "ä¸–", "ç•Œ"]
è¯è¡¨: {"ä½ ": 3, "å¥½": 4, "ä¸–": 5, "ç•Œ": 6}
```

**æ­¥éª¤ 2ï¼šç»Ÿè®¡ç›¸é‚» Token å¯¹çš„é¢‘ç‡**
```
Token å¯¹ç»Ÿè®¡:
  ("ä½ ", "å¥½") å‡ºç° 2 æ¬¡  â† æœ€é«˜é¢‘
  ("ä¸–", "ç•Œ") å‡ºç° 1 æ¬¡
```

**æ­¥éª¤ 3ï¼šåˆå¹¶é«˜é¢‘ Token å¯¹**
```
åˆå¹¶ "ä½ å¥½" â†’ æ–° tokenï¼Œåˆ†é… ID 259
è¯è¡¨æ›´æ–°: {..., "ä½ å¥½": 259}

æ–‡æœ¬é‡æ–°ç¼–ç : ["ä½ å¥½", "ä½ å¥½", "ä¸–", "ç•Œ"]
```

**æ­¥éª¤ 4ï¼šé‡å¤è¿­ä»£**
```
ç»§ç»­ç»Ÿè®¡ â†’ å‘ç° ("ä¸–", "ç•Œ") ä¹Ÿå¾ˆå¸¸è§ â†’ åˆå¹¶æˆ ID 260
...
ç›´åˆ°è¯è¡¨è¾¾åˆ°ç›®æ ‡å¤§å° 6400
```

#### æœ€ç»ˆæ•ˆæœ
```
å¸¸è§è¯/çŸ­è¯­: å•ä¸ª ID è¡¨ç¤º
  "ä½ å¥½" â†’ 259
  "ä¸–ç•Œ" â†’ 260
  "äººå·¥æ™ºèƒ½" â†’ 512
  
ç”Ÿåƒ»è¯: æ‹†åˆ†æˆå¤šä¸ªå­è¯
  "å…ƒå®‡å®™" â†’ [4521, 892]  (å…ƒ + å®‡å®™)
  
æœªè§è¿‡çš„è¯: æ‹†åˆ†åˆ°å­—èŠ‚çº§åˆ«ï¼ˆ100% è¦†ç›–ï¼‰
  "ğ•" â†’ [195, 157, 149]  (UTF-8 å­—èŠ‚åºåˆ—)
```

## 2. Global

### 2.1 ç›®æ ‡
è®¾è®¡å¹¶å®ç°ä¸€ä¸ªè‡ªå®šä¹‰çš„ä¸­æ–‡åˆ†è¯å™¨ï¼Œæ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š
- åŸºäºé¡¹ç›®çš„é¢„è®­ç»ƒæ•°æ®é›†è®­ç»ƒè¯è¡¨
- è¯è¡¨å¤§å°å›ºå®šä¸º **6400**
- å…¼å®¹ `transformers.PreTrainedTokenizer`

### 2.2 éç›®æ ‡ (Non-Goals)

å½“å‰**ä¸**åšçš„äº‹æƒ…ï¼š

- âŒ **å¤šè¯­è¨€æ”¯æŒ**ï¼šå½“å‰ä»…é’ˆå¯¹ä¸­æ–‡æ•°æ®é›†ï¼Œä¸è€ƒè™‘è‹±æ–‡ã€æ—¥æ–‡ç­‰å…¶ä»–è¯­è¨€
- âŒ **åˆ†å¸ƒå¼è®­ç»ƒ**ï¼šæ•°æ®é›†è§„æ¨¡ä¸å¤§ï¼Œå•æœºè®­ç»ƒè¶³å¤Ÿ


---

## 3. Overview

### 3.1 File

```
a_pretrain/tokenizer/
â”œâ”€â”€ design.md              # æœ¬è®¾è®¡æ–‡æ¡£
â””â”€â”€ main.py                # è®­ç»ƒå…¥å£ï¼šè®­ç»ƒå¹¶ä¿å­˜åˆ†è¯å™¨

model/
â””â”€â”€ tokenizer.json         # è®­ç»ƒäº§å‡ºï¼šåˆ†è¯å™¨é…ç½® + è¯è¡¨
```

### 3.2 Dataflow

```
Raw Dataset (HuggingFace)
    â”‚
    â”‚ [{"source": "...", "completion": "é™ˆå‡†ï¼Œå­—é“åŸº..."}, ...]
    â”‚
    â–¼
Extract Text
    â”‚
    â”‚ ["é™ˆå‡†ï¼Œå­—é“åŸº...", "å¦ä¸€ç¯‡æ–‡ç« ...", ...]
    â”‚
    â–¼
BPE Training
    â”‚
    â”‚ ç»Ÿè®¡å­—èŠ‚å¯¹é¢‘ç‡ -> åˆå¹¶é«˜é¢‘å­—èŠ‚å¯¹ -> ç”Ÿæˆè¯è¡¨
    â”‚
    â–¼
Vocabulary (6400 entries)
    â”‚
    â”‚ {0: "[PAD]", 1: "[UNK]", 2: "[EOS]", 3: "çš„", 4: "æ˜¯", ...}
    â”‚
    â–¼
Save to tokenizer.json
```
